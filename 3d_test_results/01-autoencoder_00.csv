,batchsize,timesteps,optimizer,loss,metrics,layer1,preprocess,avg_epochs,avg_loss,avg_valid_loss,avg_test_loss_0,avg_test_loss_1
0,40,16,adam,mse,mse,{'units': 5},Stable Filter,23.0,0.0013021603226661682,0.0021066453773528337,0.0024653469833234944,0.001766360558879872
1,40,16,adam,mse,mse,{'units': 10},Stable Filter,36.333333333333336,0.0008406514340701202,0.0013600304179514449,0.0012857322581112385,0.0007638104337578019
2,40,16,adam,mse,mse,{'units': 15},Stable Filter,40.0,0.0006811979110352695,0.0011023860812808077,0.0009874751752552886,0.0005471261489825944

'''
lstm -> repeat vector -> lstm
'''

from keras import Input, layers, Model
TIMESTEPS = 16
VECTOR_SIZE = 10
class Encoder_Decoder:
    def __init__(self, number_of_features):
        inputs = Input(shape=(TIMESTEPS, number_of_features))
        lstm = layers.LSTM(VECTOR_SIZE, return_sequences=False)(inputs)
        repeat = layers.RepeatVector(TIMESTEPS)(lstm)
        outputs = layers.LSTM(number_of_features, return_sequences=True)(repeat)
        self.model = Model(inputs=inputs, outputs=outputs)

    def target_function(self, data):
        x, y = data
        return x

OPTIONS = {
    "batchsize": [40],
    "timesteps": [TIMESTEPS],
    "optimizer": ["adam"],
    "loss": ['mse'],
    "metrics": ['mse'],
    "layer1": [{"units": i*5} for i in range(1, 4)],
}