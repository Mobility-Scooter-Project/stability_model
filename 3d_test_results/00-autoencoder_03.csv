,batchsize,timesteps,optimizer,loss,metrics,preprocess,avg_epochs,avg_loss,avg_valid_loss,avg_test_loss_0,avg_test_loss_1
0,40,16,adam,mae,mae,Stable Filter,16.666666666666668,0.03723750511805216,0.04193946967522303,0.04958103969693184,0.044768597930669785
1,40,16,adam,mae,mse,Stable Filter,15.333333333333334,0.03722690915067991,0.04190006603797277,0.04969500005245209,0.0449284203350544
2,40,16,adam,mse,mae,Stable Filter,16.333333333333332,0.0037220746744424105,0.004775907068202893,0.005962593170503776,0.0053519670230646925
3,40,16,adam,mse,mse,Stable Filter,21.0,0.0037151515328635774,0.004773264129956563,0.005988012999296188,0.0053799535768727464

from keras import Input, layers, Model

TIMESTEPS = 16
VECTOR_SIZE = 10

class Encoder_Decoder:
    def __init__(self, number_of_features):

        inputs = Input(shape=(TIMESTEPS, number_of_features))
        x = layers.Conv1D(1, 3, padding="same")(inputs)
        outputs = layers.Conv1DTranspose(number_of_features, 3, padding="same")(x)
        self.model = Model(inputs=inputs, outputs=outputs)

    def target_function(self, data):
        x, y = data
        return x

OPTIONS = {
    "batchsize": [40],
    "timesteps": [TIMESTEPS],
    "optimizer": ["adam"],
    "loss": ['mae', 'mse'],
    "metrics": ['mae', 'mse'],
    # "layer1": [{"units": i*5} for i in range(1, 10)],
}