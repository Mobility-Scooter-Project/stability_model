,batchsize,timesteps,optimizer,loss,metrics,layer2,preprocess,avg_epochs,avg_loss,avg_valid_loss,avg_test_loss_0,avg_test_loss_1
0,40,16,adam,mse,mse,{'units': 3},Stable Filter,40.0,0.0005886088474653661,0.0010208198412631948,0.001202870315561692,0.0009211592259816825
1,40,16,adam,mse,mse,{'units': 6},Stable Filter,40.0,0.0002812146849464625,0.0004812415669827412,0.0007849869628747305,0.000656489321651558
2,40,16,adam,mse,mse,{'units': 9},Stable Filter,38.333333333333336,0.00012249533513871333,0.00020424736430868506,0.0002986303491828342,0.0003059440835689505

'''
lstm -> lstm -> lstm
'''
from keras import Input, layers, Model

TIMESTEPS = 16
VECTOR_SIZE = 3
class Encoder_Decoder:
    def __init__(self, number_of_features):
        inputs = Input(shape=(TIMESTEPS, number_of_features))
        lstm = layers.LSTM(64, return_sequences=True)(inputs)
        lstm = layers.LSTM(VECTOR_SIZE, return_sequences=True)(lstm)
        lstm = layers.LSTM(64, return_sequences=True)(lstm)
        outputs = layers.LSTM(number_of_features, return_sequences=True)(lstm)
        self.model = Model(inputs=inputs, outputs=outputs)

    def target_function(self, data):
        x, y = data
        return x

OPTIONS = {
    "batchsize": [40],
    "timesteps": [TIMESTEPS],
    "optimizer": ["adam"],
    "loss": ['mse'],
    "metrics": ['mse'],
    "layer2": [{"units": i*3} for i in range(1, 4)],
}