,batchsize,timesteps,optimizer,loss,metrics,layer1,preprocess,avg_epochs,avg_loss,avg_valid_loss,avg_test_loss_0,avg_test_loss_1
0,40,16,adam,mse,mse,{'units': 1},No Change,19.666666666666668,0.005224273540079594,0.005768151488155127,0.018361935392022133,0.010695224006970724
1,40,16,adam,mse,mse,{'units': 2},No Change,24.0,0.0037374873645603657,0.004500057393064101,0.005686924171944459,0.003369609902923306
2,40,16,adam,mse,mse,{'units': 3},No Change,26.333333333333332,0.002779153330872456,0.0030585615895688534,0.004268523460874955,0.0024913078329215446
3,40,16,adam,mse,mse,{'units': 4},No Change,31.0,0.0020209495754291615,0.0023458697833120823,0.0032004958484321833,0.0018770667569090922
4,40,16,adam,mse,mse,{'units': 5},No Change,28.666666666666668,0.0013473618309944868,0.0017106704569111268,0.0023481317330151796,0.0012835537393887837

from keras import Input, layers, Model

TIMESTEPS = 16
VECTOR_SIZE = 10

class Encoder_Decoder:
    def __init__(self, number_of_features):
        inputs = Input(shape=(TIMESTEPS, number_of_features))
        lstm = layers.LSTM(VECTOR_SIZE, return_sequences=True)(inputs)
        outputs = layers.Conv1DTranspose(number_of_features, 3, padding="same")(lstm)
        self.model = Model(inputs=inputs, outputs=outputs)

    def target_function(self, data):
        x, y = data
        return x

OPTIONS = {
    "batchsize": [40],
    "timesteps": [TIMESTEPS],
    "optimizer": ["adam"],
    "loss": ['mse'],
    "metrics": ['mse'],
    "layer1": [{"units": i} for i in range(1, 6)],
}